{"cells":[{"cell_type":"markdown","source":["# **Part 0: Introduction to Yelp Dataset Project**"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["### Part 0.0: Yelp Academic Dataset\n\nThis project delves into exploratory analysis and building predictive models using the [Yelp academic dataset](https://www.yelp.com/dataset_challenge/). It is an opportunity for you to explore a machine learning problem in the context of a real-world data set using big data analysis tools. In order to use the dataset and finish this project, you must agree to the dataset's terms of use provided [here](https://www.yelp.com/html/pdf/Dataset_Challenge_Academic_Dataset_Agreement.pdf).\n\nWe have chosen a subset of the Yelp academic dataset for you to work with. This subsampled data is loaded into RDDs in section (1). The complete dataset is available from Yelp's website [here](https://www.yelp.com/dataset_challenge/dataset). Remember that you are limited by the DataBricks Community Edition's limits on memory and computation. Yelp has provided some example code at their Github repository [here](https://github.com/Yelp/dataset-examples) that might be helpful in getting started. However, these are pure Python code and not Spark code that provide parallelism.\n\nBy design, the project is open-ended; you are free to decide how you want to approach the problem and what tools you want to employ. We want to see a best-effort solution that utilizes what you learned in class and also potentially trying new things beyond class. Your project will be worth 20% of your final class grade."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["### Part 0.1: Grading Rubric:\n\n** Course staff will use the following rubric when grading your final project reports: **\n\n\n*  *Introduction/Motivation/Problem Definition (10%)*\n  * Identify, define, and motivate the problem that you are addressing.\n  * How (precisely) will a machine learning solution address the problem?\n\n*  *Data Understanding and Preparation (15%)*\n  * What preliminary analyses have you performed on the data? What observations have you made? How did those observations help shape your approach?\n  * Provide the preliminary data analysis results and your observations.\n  * Specify how the data will be transformed to the format required for machine learning. \n\n*  *Methodology (35%)*\n  * This is where you give a detailed description of your primary contributions. It is especially important that this part be clear and well written so that we can fully understand what you did.\n  * Specify the type of model(s) built and/or information/knowledge extracted.\n  * Discuss choices for machine learning algorithm: what are other alternatives, and what are their pros and cons (in the context of the problem and as compared to your proposed solution)?\n  * Discuss why and how this model should \"solve\" the problem (i.e., improve along some dimension of interest). \n  * Outline the big data analysis tools and libraries you have used. \n\nIt is not so important how well your method performs but rather, (a) how thorough and careful your methodology is, and (b) how interesting and clever the approaches you took and the tools you have used are. \n\n*  *Evaluation and Results (30%)*\n  * We are interested in seeing a clear and conclusive set of experiments which successfully evaluate the problem you set out to solve. Make sure to interpret the results and talk about what we can conclude and learn from your approach.\n  * How do you evaluate your machine learning solution to the specific question(s) you have addressed?\n  * What do these results tell you about your solution?\n  * Present and discuss your evaluation results and findings. You may use tables or figures (e.g. ROC plot) to visualize your results.\n\n*  *Style and writing (10%)*\n  * Overall writing, grammar, organization, figures and illustrations.\n \nNote that, for reference, you can look up the details of the relevant Spark methods in [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) and the relevant NumPy methods in the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["### ** Part 0.2: Code of Conduct **\n\n** Please follow the following guidelines with respect to collaboration: **\n\n* You have to use the data we have provided you. You cannot choose your own dataset. By using the dataset, you agree to Yelp's terms of use available [here](https://www.yelp.com/html/pdf/Dataset_Challenge_Academic_Dataset_Agreement.pdf).\n* You will be given 48 hours to work on the project. Use of late days are not allowed for this submission.\n* You are free to use the Web, APIs, ML toolkits, etc. in this project to your best benefit. Please credit any online or offline sources (even casual sources like StackOverflow) if you use them in the project.\n* Project is to be done individually. No collaboration is allowed between students. No discussion is allowed about the project with anyone else except the class instructors. Students who use each other's ideas or code will be heavily penalized."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["### **Part 0.3: Project Suggestions **\n\nBefore you embark on the project, please plan out your task by breaking it into smaller chunks that incrementally build on top of each other. For example, you may begin with a simpler set of features and then add more complex features to the dataset. Such modular planning will ensure that you will have a working deliverable in case you run out of time tackling more complicated aspects of the project you had planned to complete. Try to have a barebones but working version of the project after 24 hours, and build on it in the next 24 hours leading up to the deadline. Create a backup version of your notebook after finishing a substantial chunk of the work so you can go back to a working version in case of a catastrophe.\n\n**Here is a list of potential aspects you can tackle in this final open-ended project:**\n\n*  *Exploratory Data Analysis (Perform those that help you get started with your chosen business question, not all of these.)*\n  * Plot a map showing the locations of various businesses. Helper code to help in the creation of maps using \"mpl_toolkits\" is provided in a later section on exploratory data analysis.\n  * Plot a map showing the locations of businesses checkins made by Yelp users.\n  * Plot a histogram of ratings that the businesses get i.e. see how many businesses got ratings of 1-5 each. Is this distribution skewed? Are there ratings that are used rarely as compared to others?\n  * What are the most popular keywords that reviewers use by city or state?\n  * What are the most popular keywords that reviewers use for American/Thai/Chinese restaurants by state?\n  * What are the most popular keywords or adjectives that reviewers use for American/Thai/Chinese restaurants by state?\n  * What are the most popular keywords or adjectives that reviewers use in 5 star reviews for American/Thai/Chinese restaurants by state?\n  * What are the most frequent keywords or adjectives that reviewers use in 1 star reviews for American/Thai/Chinese restaurants by state?\n  * Does the distribution of restaurants with parking space or outdoor seating differ from state to state or city to city?\n  * Are there temporal trends (daily, weekly, holidays) associated with business checkins?\n  * Does the number of checkins per restaurant differ across various restaurant categories?\n  * Is there a correlation between how long a user has been \"yelping\" and the number of reviews he has written?\n  * Is there a correlation between how many friends/fans a user has and the number of votes his reviews get?\n  * What are the 5 most common types of restaurants in each city?\n  * What is the fraction of businesses that accounts for restaurants?\n  * Do the typical business hours vary by city and by type of business?\n  * What does the histogram of number of friends/fans of Yelp users look like? Is it long-tailed or does it follow a certain distribution?\n  * What is the distribution of number of reviews by neighborhood?\n  * Is there a correlation between the star rating and length of reviews?\n  * What are the top keywords or adjectives used by the two genders (male and female, sorry for being binary) in their reviews?\n  * What fraction of Yelp users is male? What fraction is female? What if the fraction of users for whom gender cannot be determined based on the list of male and female names provided in this notebook?\n  * What is the average number of friends/fans for male and female users?\n  * What is the average number of reviews written by male and female users?\n*  *Classification (Any classification task should also include a description of all the features used and which of these features impacted classification performance the most and why.)*\n  * Classify businesses into various business categories (restaurant, dry cleaner, auto body, etc.).\n  * Classify businesses by the type of parking they provide (street, garage, valet, etc.).\n  * Predict the location of a reviewer (east or west coast or mid-country).\n  * Predict the location of a business (east or west coast or mid-country).\n  * Predict if a review is funny, cool, or useful (label should be based on the corresponding votes associated with the review, votes therefore may not be used as features).\n  * Predict which type of restaurant a user reviews most based on the restaurant types reviewed by his friends.\n  * Given the current categories of a business, predict a new category that it could be labeled as. You will need businesses - each with mutiple categories - to hold out some categories randomly from each business for testing purposes.\n  * Predict if two users are friends based on the locations of businesses for which they have written reviews and other user characteristics.\n  * Based on businesses reviewed by a user until a certain timepoint, predict the type of business the user might review next.\n  * Predict if the ratings for a business are going to increase or decline with time. Are some types of restaurants more inclines to suffer from declining ratings?\n  * Predict the gender of Yelp user based on businesses they have written reviews for. Examine a few examples of Yelp users where your classifier is incorrect, and provide any insighful suggestions for improving the classifier.\n  * Predict the gender of Yelp users based on their business reviews. Examine your model to determine if and how the two genders use different words when writing reviews.\n  * Predict the gender of Yelp users based on the numbers of various types of votes their reviews get and the numbers of various types of compliments they receive. Examine a few examples of Yelp users where your classifier is incorrect, and provide any insighful suggestions for improving the classifier.\n*  *Regression (Any regression task should also include a description of all the features used and which of these features impacted regression performance the most and why.)*\n  * Predict the average rating of a business from its reviews and other business characteristics such as location.\n  * Predict the total number of reviews on a given week for each business.\n  * Predict the total number of checkins based on business location, type, and other business characteristics present in \"attributes\" such as \"Happy Hour, \"Accepts Credit Cards\", \"Good For Groups\", \"Outdoor Seating\", and \"Price Range\".\n  * Predict the number of compliments received by a user.\n  * Predict the number of friends a user has on Yelp based on user characteristics like number of reviews written by him, compliments received, etc.\n  * Based on reviews written by a user until a certain timepoint, predict the star rating the user will give as part of his next review. Are certain users more likely to give extreme ratings to reviewed businesses than others?\n  * Predict the number of funny/cool/useful votes sent by a user. Does it depend significantly on how long the user has been \"yelping\" or on gender?\n  * Predict the number of funny/cool/useful votes received by a review. Does it depend significantly on how long the user has been \"yelping\" or on gender?\n* *Clustering*\n  * Cluster business by using their features using a clustering algorithm such as [K-Means](https://spark.apache.org/docs/2.1.0/mllib-clustering.html#k-means). Choose the number of clusters in a data-driven fashion such as by using the elbow heuristic. Analyze clusters and see if they are homogeneous i.e. the business within each cluster look similar and as if they belong within the same group.\n  * Cluster users based on their characteristics. See if users in the same cluster patronize similar businesses.\n* *Recommendation Systems*\n  * Given previous ratings by a Yelp user, recommend other businesses that the user might like. See [Collaborative Filtering on Spark](https://spark.apache.org/docs/2.1.0/mllib-collaborative-filtering.html).\n  * You may also think of restaurant/business recommendation as a link prediction problem. You can use GraphX for this task.\n* *Discovering Insights using Unsupervised Algorithms*\n  * In the Yelp user dataset, you are provided a social network in the form of friends of each Yelp user. You may perform social network analysis using GraphX. This can help you discover the most influential users by eigen-centrality. You may use other measures of network centrality besides eigen-centrality.\n  * Discover dense clusters of closely connected friends and see if they patronize the same businesses.\n  * Verify if the dense clusters of closely connected friends are also homogeneous in terms of gender.\n  * Learn a set of topics by applying topic modeling algorithms such as [LDA](https://spark.apache.org/docs/2.1.0/mllib-clustering.html#latent-dirichlet-allocation-lda) on textual reviews of businesses. Choose the number of topics in a data-driven fashion such as by using a figure that plots perplexity versus number of topics. Explore if the topics are insightful and whether or not they can be used as inputs to some predictive algorithms (see Classification tasks above).\n  * Perform the above topic modeling precedure on the reviews of male and female reviewers separately to obtain two topic models. Explore if the topics are insightful and/or useful in any predictive tasks.\n  * Apply PCA to the matrix where rows are businesses and columns are features of the businesses such as parking type, location, etc. Choose the number of components in a data-driven fashion such as by using a scree plot. Explore if the top components are insightful and can be used as inputs to any predictive algorithms (see classification tasks above).\n  \nYour task is to choose one of these ML problems, or define your own, on the provided dataset and address the problem of your choice with the big data analysis tools you learned during the course as well as others you explore based on the APIs in Spark. If you create your own question, please be sure to state it clearly at the beginning of section 4 (methodology)."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["### ** Part 0.4: Setup your DataBricks CE Spark Cluster and IPython Notebook **\n\nStep I: Visit the web interface of DataBricks Community Edition at https://community.cloud.databricks.com/.\n\nStep II: Start a DataBricks Community Edition cluster by selecting \"New Cluster\" from the homepage.\n\nStep III: Give your cluster a name and click on \"Create Cluster\". This creates a single node cluster with 6GB memory for your account.\n\nStep IV: Go back to homepage, and choose \"Import Notebook.\" Upload the IPython assignment notebook by following the prompts and open the notebook. Rename the notebook from \"project_yelp_dataset.ipynb\" to \"andrewid_project_yelp_dataset.ipynb\" where \"andrewid\" is your actual Andrew ID.\n\nStep V: By default, the notebook is not attached to a Spark cluster and will show \"Detached\" as its status at the top of the notebook in the browser. Click on the \"Detached\" status and attach it to your cluster. It should now show the message as \"Attached (cluster name)\"\n\nStep VI: You can now import pyspark into the notebook. Also, attaching to the DataBricks Community Edition cluster automatically provides the SparkContext variable \"sc\" to the Python code in your notebook. Use it to create RDDs and write further Spark code.\n\nThese instructions are detailed with screenshots in slides 10-16 of the setup recitation available at https://www.andrew.cmu.edu/user/amaurya/docs/95869/hadoop-spark-setup-recitation.pdf"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["### ** Part 0.5: Submission Instructions **\n\nYou will submit both a zipped file on Blackboard and a hardcopy to the TA Abhinav Maurya in HBH 3026. If the TA's office is closed, please slide the hardcopy under the door.\n\nPlease complete the project, and feel free to add new cells as required. Upon completion, execute all cells in the completed notebook, and make sure all results show up. Export the contents of the notebook by choosing \"File > Export > HTML\" and saving the resulting file as \"andrewid_project_yelp_dataset.html\" Place the two files \"andrewid_project_yelp_dataset.ipynb\" and \"andrewid_project_yelp_dataset.html\" in a folder, zip the folder to a zipped file named \"andrewid_project.zip\" and submit it to Blackboard by the deadline. In addition, print the HTML file and submit the hardcopy to the TA Abhinav Maurya in HBH 3026 by the deadline. If the TA's office is closed, please slide the hardcopy under the door."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["# ** Part 1: Load the datasets required for the project **"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["We will load four datasets for this project. Please feel free to reasonably subsample the dataset depending on the question you are answering and its complexity. If you choose to subsample any of the four datasets, please explain why you subsampled it and what was the number of datapoints you were left with in the subsampled version. In addition to the four datasets, we will also load two lists which contain names by gender. These lists are helpful in assigning a gender to a Yelp user by their name, since gender is not available in the Yelp dataset."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["import json\nimport os\nimport sys\nimport os.path\nimport pyspark\nimport urllib2\nimport numpy as np\n\n# helper function to load a JSON dataset from a publicly accessible url\ndef get_rdd_from_url(url):\n  response = urllib2.urlopen(url)\n  str_contents = response.read().strip().split('\\n')\n  json_contents = [json.loads(x) for x in str_contents]\n  rdd = sc.parallelize(json_contents)\n  return rdd"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["The first dataset we are going to load is information about Yelp businesses. The information of each business will be stored as a Python dictionary within an RDD. The dictionary consists of the following fields:\n\n* \"business_id\":\"encrypted business id\"\n* \"name\":\"business name\"\n* \"neighborhood\":\"hood name\"\n* \"address\":\"full address\"\n* \"city\":\"city\"\n* \"state\":\"state -- if applicable --\"\n* \"postal code\":\"postal code\"\n* \"latitude\":latitude\n* \"longitude\":longitude\n* \"stars\":star rating, rounded to half-stars\n* \"review_count\":number of reviews\n* \"is_open\":0/1 (closed/open)\n* \"attributes\":[\"an array of strings: each array element is an attribute\"]\n* \"categories\":[\"an array of strings of business categories\"]\n* \"hours\":[\"an array of strings of business hours\"]\n* \"type\": \"business\""],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["# load the data about Yelp businesses in an RDD\n# each RDD element is a Python dictionary parsed from JSON using json.loads()\n# if your chosen project does not need this data, please comment out the lines below\nbusinesses_rdd = get_rdd_from_url('https://www.andrew.cmu.edu/user/amaurya/docs/95869/yelp_academic_dataset_business.json')\nprint businesses_rdd.count()\nprint businesses_rdd.take(2)"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["The second dataset we are going to load is information about Yelp users. Each user's information will be stored as a Python dictionary within an RDD. The dictionary consists of the following fields:\n\n*  \"user_id\":\"encrypted user id\"\n*  \"name\":\"first name\"\n*  \"review_count\":number of reviews\n*  \"yelping_since\": date formatted like \"2009-12-19\"\n*  \"friends\":[\"an array of encrypted ids of friends\"]\n*  \"useful\":\"number of useful votes sent by the user\"\n*  \"funny\":\"number of funny votes sent by the user\"\n*  \"cool\":\"number of cool votes sent by the user\"\n*  \"fans\":\"number of fans the user has\"\n*  \"elite\":[\"an array of years the user was elite\"]\n*  \"average_stars\":floating point average like 4.31\n*  \"compliment_hot\":number of hot compliments received by the user\n*  \"compliment_more\":number of more compliments received by the user\n*  \"compliment_profile\": number of profile compliments received by the user\n*  \"compliment_cute\": number of cute compliments received by the user\n*  \"compliment_list\": number of list compliments received by the user\n*  \"compliment_note\": number of note compliments received by the user\n*  \"compliment_plain\": number of plain compliments received by the user\n*  \"compliment_cool\": number of cool compliments received by the user\n*  \"compliment_funny\": number of funny compliments received by the user\n*  \"compliment_writer\": number of writer compliments received by the user\n*  \"compliment_photos\": number of photo compliments received by the user\n*  \"type\":\"user\""],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["# load the data about Yelp users in an RDD\n# each RDD element is a Python dictionary parsed from JSON using json.loads()\n# if your chosen project does not need this data, please comment out the lines below\nusers_rdd = get_rdd_from_url('https://www.andrew.cmu.edu/user/amaurya/docs/95869/yelp_academic_dataset_user.json')\nprint users_rdd.count()\nprint users_rdd.take(2)"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["The third dataset we are going to load is information about business checkins reported by users on Yelp. Each checkin's information will be stored as a Python dictionary within an RDD. The dictionary consists of the following fields:\n\n*  \"checkin_info\":[\"an array of check ins with the format day-hour:number of check ins from hour to hour+1\"]\n*  \"business_id\":\"encrypted business id\"\n*  \"type\":\"checkin\""],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["# load the data about business checkins reported by users on Yelp in an RDD\n# each RDD element is a Python dictionary parsed from JSON using json.loads()\n# if your chosen project does not need this data, please comment out the lines below\ncheckins_rdd = get_rdd_from_url('https://www.andrew.cmu.edu/user/amaurya/docs/95869/yelp_academic_dataset_checkin.json')\nprint checkins_rdd.count()\nprint checkins_rdd.take(2)"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["The fourth dataset we are going to load is information about business reviews written by users on Yelp. Each review's data will be stored as a Python dictionary within an RDD. The dictionary consists of the following fields:\n\n*  \"review_id\":\"encrypted review id\"\n*  \"user_id\":\"encrypted user id\"\n*  \"business_id\":\"encrypted business id\"\n*  \"stars\":star rating rounded to half-stars\n*  \"date\":\"date formatted like 2009-12-19\"\n*  \"text\":\"review text\"\n*  \"useful\":number of useful votes received\n*  \"funny\":number of funny votes received\n*  \"cool\": number of cool review votes received\n*  \"type\": \"review\""],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["# load the data about business reviews written by users on Yelp in an RDD, limited to businesses in Pittsburgh due to DataBricks computational limits\n# each RDD element is a Python dictionary parsed from JSON using json.loads()\n# if your chosen project does not need this data, please comment out the lines below\nreviews_rdd = get_rdd_from_url('https://www.andrew.cmu.edu/user/amaurya/docs/95869/yelp_academic_dataset_review_pittsburgh.json')\nprint reviews_rdd.count()\nprint reviews_rdd.take(2)"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Finally, we will load two lists. The first list consists of male names, and the second list consists of female names. You can use these lists to predict the gender of Yelp users if you plan to do any gender-based analysis of users or their reviews."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["# helper function to load a list of names from a publicly accessible url\ndef get_names_from_url(url):\n  response = urllib2.urlopen(url)\n  str_contents = response.read().strip().split('\\n')\n  result = str_contents[6:]\n  return result\n\nmale_names = get_names_from_url('https://www.andrew.cmu.edu/user/amaurya/docs/95869/male.txt')\nprint('First five male names: ', male_names[:5])\n\nfemale_names = get_names_from_url('https://www.andrew.cmu.edu/user/amaurya/docs/95869/female.txt')\nprint('First five female names: ', female_names[:5])"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["# ** Part 2: Introduction, Motivation, and Problem Definition **"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["Please write your answer here. Add additional IPython code/markup cells as needed. Please the grading rubric at the top of this notebook to understand expectations from this section.\n\nDescribe your chosen problem and why you think it is interesting from a business perspective. Also mention which of the four datasets you will use for the analysis. What metric(s) will you use to evaluate methods on your chosen task?"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["The chosen problem for my study was: \n\n* \"Predict the gender of Yelp user based on businesses they have written reviews for. Examine a few examples of Yelp users where your classifier is incorrect, and provide any insighful suggestions for improving the classifier\"*\n\nPurpose: The problem is about predicting the gender of the Yelp user who are putting reviews on businesses. This task will enable us to get insights into the demographic inofrmation about the population who is active on reviwing the various types of businesses and also would enable us to know whether more number of females or males are taking intersest in which type of business. Also, which business is liked most by each gender type will be visualized by the gender distribution which can  reveal hidden user demographics and showcase product success or failure within the groups. Gender based Yelp data analysis will also trigger a mechanism where we can detect which busnesses needs improvement or which businesses are doing great in terms of reviews and their location. Being able to accurately predict gender from businesses data would provide valuable information for marketing, sales and evaluation.\n\nChosen Datasets: The datasets which are of interest and important for our observation, model development and analysis are: businesses, reviews, user and gender.\n\nFor evaluation puposes, I will first join/explore the datasets to get it in desired (features, label) format. Then quickly try different binary classification methods to gauge which one best suites the dataset. While deciding which method suites best, I plan to use log-loss & accuracy metrics. Once I decide on a model that seems to work well, I will try to improve it. To improve the model I will do hyperparameter tuning, apply any dataset specific fix (like down-sampling/up-sampling samples from a specific category), etc. Also, as the given task requires me to perform a classification based Machine Learning approach, I will be using MLlib extensively over pyspark for my model building and python libraries like Matplotlib & Numpy."],"metadata":{}},{"cell_type":"markdown","source":["# ** Part 3: Data Understanding and Preparation **"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["Please write your answer here. Add additional IPython code/markup cells as needed. Please the grading rubric at the top of this notebook to understand expectations from this section.\n\nDescribe your exploratory data analysis in this section. This is really important because it establishes that the datasets you are exploring is capable of answering your chosen project question. Make this section rich with visualization to give the reader a comprehensive understanding of the datasets you have chosen to use.\n\nBelow, you are provided helper code to install matplotlib's extra toolkit \"mpl_toolkits\" required for drawing maps. Also provided is an example map created using mpl_toolkits. You can refer to Matplotlib Basemap Toolkit documentation [here](https://matplotlib.org/basemap/)."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["%sh -e\n\n# shell commands to install mpl_toolkits\nsudo pip install matplotlib\ncd /databricks\nmkdir -p mpl_toolkit\ncd mpl_toolkit\n\nwget https://www.andrew.cmu.edu/user/amaurya/docs/95869/basemap-1.0.7.tar.gz\ntar -xvf basemap-1.0.7.tar.gz\n\ncd basemap-1.0.7/geos-3.3.3\nexport GEOS_DIR=/usr/local\n./configure --prefix=$GEOS_DIR\nmake; make install\ncd ..\npython setup.py install"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport mpl_toolkits\nmpl_toolkits.__path__.append('/usr/local/lib/python2.7/dist-packages/mpl_toolkits/')\nfrom mpl_toolkits.basemap import Basemap\n\n\ndef preparePlot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n                gridWidth=1.0):\n    \"\"\"Template for generating the plot layout.\"\"\"\n    plt.close()\n    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n        axis.set_ticks_position('none')\n        axis.set_ticks(ticks)\n        axis.label.set_color('#999999')\n        if hideLabels: axis.set_ticklabels([])\n    plt.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n    return fig, ax"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":["fig, ax = preparePlot(np.arange(0, 100, 20), np.arange(0, 100, 20))\nm = Basemap(projection='merc',llcrnrlat=-80,urcrnrlat=80, llcrnrlon=-180,urcrnrlon=180,lat_ts=20,resolution='c')\nm.drawcoastlines()\nm.fillcontinents(color='coral',lake_color='aqua')\nm.drawparallels(np.arange(-90.,91.,30.))\nm.drawmeridians(np.arange(-180.,181.,60.))\nm.drawmapboundary(fill_color='aqua')\nplt.title(\"Mercator Projection\")\ndisplay(fig)"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":["fig, ax = preparePlot(np.arange(0, 100, 20), np.arange(0, 100, 20))\nm = Basemap(width=12000000,height=9000000,projection='lcc', resolution=None,lat_1=45.,lat_2=55,lat_0=50,lon_0=-107.)\nm.bluemarble()\ndisplay(fig)"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# Data Visualization & Preliminary analysis\n\n# Start with storing the required RDDs into memory. Since we will\n# be using them over and over again, its best to cache them to speed\n# up computations.\nusers_rdd.cache()\nreviews_rdd.cache()\nbusinesses_rdd.cache()\ncheckins_rdd.cache()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# Since different datasets have same columns names, to avoid confusion\n# I prefix all the columns with the dataset identifier. This way when we\n# do joins in the following steps, all the column are uniquely identified.\ndef prefix_columns(df, prefix):\n  field_name = map(lambda field: field.name, df.schema.fields)\n  for name in field_name:\n    if name.startswith(prefix):\n      continue\n      \n    df = df.withColumnRenamed(name, prefix + name)\n    \n  return df"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# Neighborhoods has mixed types data (bool & string) and \n# messes up spark auto schema infer.\ndef drop_neighborhoods(v):\n  del v[\"neighborhoods\"]\n  return v\n\nusers_df = prefix_columns(spark.createDataFrame(users_rdd), \"user_\")\nreviews_df = prefix_columns(spark.createDataFrame(reviews_rdd), \"review_\")\ncheckins_df = prefix_columns(spark.createDataFrame(checkins_rdd), \"checkin_\")\nbusinesses_df = prefix_columns(spark.createDataFrame(businesses_rdd.map(drop_neighborhoods)), \"business_\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["MALE = \"MALE\"\nFEMALE = \"FEMALE\"\nUNKNOWN = \"UNKNOWN_GENDER\"\n\nusers_gender_by_name = {}\nfor name in male_names:\n  users_gender_by_name[name] = MALE\nfor name in female_names:\n  users_gender_by_name[name] = FEMALE\n    \nusers_gender_by_name_df = spark.createDataFrame(users_gender_by_name.items(), (\"user_name\", \"gender\"))\nprint users_gender_by_name_df.take(2)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# We are missing gender for some user names, so I do left_outer join here to retain those rows in the output.\ndf = users_df.join(users_gender_by_name_df, users_df.user_name == users_gender_by_name_df.user_name, 'left_outer')\ndf = df.join(reviews_df, df.user_id == reviews_df.review_user_id)\ndf = df.join(businesses_df, df.review_business_id == businesses_df.business_id)\ndf.cache()\n\nprint df.count()\nprint df.take(1)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# Here is a plot showing number of business review grouped by \n# user gender.\n#\n# We notice that the dataset is highly skewed towards females.\n# Therefore we should expect that model will be biased towards\n# females and will make false positive predictions for it.\n#\n# Later in the evaluation section, I will try to handle this anamoly.\n\nfrom matplotlib import pyplot as plt\n \nplt.close()\nby_gender = df.groupBy('gender').count().collect()\ncategories = [i[0] for i in by_gender]\ncounts = [i[1] for i in by_gender]\n \nind = np.array(range(len(categories)))\nwidth = 0.50\nplt.bar(ind, counts, width=width, color='r')\n \nplt.ylabel('reviews counts')\nplt.title('Review count by gender')\nplt.xticks(ind + width/2., categories)\ndisplay(plt.gcf())"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["#Import numpy, pandas, and ggplot\nfrom ggplot import *\nfrom datetime import datetime\nfrom pyspark.sql.functions import udf"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# 1. Day of week vs reveiw count broken down by gender\nto_day = udf(lambda x: datetime.strptime(x, '%Y-%m-%d').strftime(\"%A\"))\nday_gender_df = df.select(df.gender, to_day(df.review_date).alias('day')).groupby(\"gender\", \"day\").count()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# This suggests females consistently review more business than males on any day of the week\np = ggplot(day_gender_df.toPandas(), aes('day', 'count', fill='gender')) + geom_bar(stat=\"identity\")\ndisplay(p)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# 2. Review ratings by city\nreviews_by_city = df.groupby('business_city').count()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# Majority of data samples are from Pittsburgh. Later any observation about city can be ignored.\np = ggplot(reviews_by_city.toPandas(), aes('business_city', 'count')) + geom_bar(stat=\"identity\") + scale_y_log() + theme(axis_text_x  = element_text(angle = 45, hjust = 1))\ndisplay(p)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# 3. review star rating distribution by gender\nreview_stars_by_gender = df.groupby('review_stars', 'gender').count()"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["p = ggplot(review_stars_by_gender.toPandas(), aes(x='gender', y='count', fill='review_stars')) + geom_bar(stat=\"identity\")\ndisplay(p)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["# Here I define features I am going to use for our prediction model. I make all the\n# features categorical.\n#\n# These include - \n# * business attributes (all attributes are treated as top level feature)\n# * business hours (made categorical by making <day>-<open/close> as top level feature)\n# * business city\n# * business name\n# * business stars (this is categorical since there are only 9 possible values of it)\n# * business state\n# * business longitude/latitude (made categorical by using them to map business on a grid)\n# * business review count (made categorical by taking log(10))\n#\n# And user gender is treated as label.\n\n# Making all the business attributes a top level column\ndef extract_business_attributes(row):\n  names = []\n  for attr in row['attributes'].iterkeys():\n    names.append(attr)\n  return names\n\nbusiness_attributes = businesses_rdd.flatMap(extract_business_attributes).distinct().collect()\n#print business_attributes[:5]\n\ndef extract_business_hours(row):\n  names = []\n  for day, kv in row['hours'].iteritems():\n    for k in kv.iterkeys():\n      names.append(day + '-' + k)\n  return names\n\nbusiness_hours = businesses_rdd.flatMap(extract_business_hours).distinct().collect()\n#print business_hours[:5]\n\nbusiness_top_level_features = [\n  'business_city',\n  'business_name',\n  'business_stars',\n  'business_state',\n  'business_latitude',\n  'business_longitude',\n  'business_review_count',\n]\n\nbusiness_features = business_attributes + business_hours + business_top_level_features\n\nbusiness_features_index = {}\nfor idx, feature in enumerate(business_features):\n  business_features_index[feature] = idx\n  \n# Only using features from businesses users have written reviews for.\nprint \"Total number of features:\", len(business_features)\nprint \"Sample features:\", business_features[:5]"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["From above, I observed that few business features were having distinct and definite levels, so I flattened those attributes and assigned their respective values without effecting the actual row count."],"metadata":{}},{"cell_type":"code","source":["from math import log\n\n# Converts a row from joined dataset (where some values are nested dictionaries)\n# to a pair of (label, features).\ndef flatten(row):\n  label = row['gender']\n  \n  features = [None] * len(business_features)\n  for attr, value in row['business_attributes'].iteritems():\n    features[business_features_index[attr]] = value\n  \n  for day, kv in row['business_hours'].iteritems():\n    for k, v in kv.iteritems():\n      feature = day + '-' + k\n      features[business_features_index[feature]] = v\n  \n  for feature in business_top_level_features:\n    val = row[feature]\n    \n    # For latitude/longitude we kind of divide location in grid by taking\n    # only int component. We can make finer grid for better results.\n    if feature == 'business_latitude':\n      val = int(val)\n    elif feature == 'business_longitude':\n      val = int(val)\n    elif feature == 'business_review_count':\n      # For review_count we take log, which kind of divides business in \n      # fewer groups - (0-9) very few review, (10-99) few review, \n      # (100-999) good number of reviews, etc.\n      val = int(log(val + 1, 10))\n      \n    features[business_features_index[feature]] = val\n  \n  return (label, features)\n\nrawRdd = df.rdd.map(flatten)\nrawRdd.cache()\nprint df.take(1)\nprint business_features\nprint rawRdd.take(1)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["Here, we will employ One-Hot-Encoding technique on categorical features to numerical features to make our dataset ready for classification."],"metadata":{}},{"cell_type":"code","source":["# I am going to use OHE on categorical features ignoring None values.\n# OHE helps us map features like 'BYOB', 'Take-out', etc to integer value for analysis.\noheFeatures = rawRdd \\\n  .flatMap(lambda (label, features): zip(business_features, features)) \\\n  .filter(lambda (key, value): value is not None) \\\n  .distinct()\n\n# Features with top distinct values\nprint oheFeatures.mapValues(lambda x: 1).reduceByKey(lambda acc, val: acc + val).top(5, key=lambda x: x[1])\n\n# Total number of OHE features\nprint \"Total number of OHE features:\", oheFeatures.count()\nprint \"Sample OHE features:\", oheFeatures.take(5)\n\n# OHE indexes for all (categorical features, values) & ordinal values\noheDict = {}\nfor idx, oheFeature in enumerate(oheFeatures.collect()):\n  oheDict[oheFeature] = idx"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["# Data points can typically be represented with a small number of non-zero \n# OHE features relative to the total number of features that occur in the dataset.\n# By leveraging this sparsity and using sparse vector representations of OHE data, \n# I can reduce storage and computational burdens. Therefore, I use SparseVectors \n# for our newly constructed LabeledPoints with gender as our label and rest all \n# others as our feature set.\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.linalg import SparseVector\n\ndef extract_label(row):\n  gender = row[0]\n  if gender == MALE:\n    return 0\n  elif gender == FEMALE:\n    return 1\n  else:\n    return 2\n\ndef extract_features(row):\n  d = {}\n  \n  for feature in business_features:\n    idx = business_features_index[feature]\n    val = row[1][idx]\n    if val is not None:\n      d[oheDict[(feature, val)]] = 1\n    \n  return SparseVector(len(oheDict), d)\n\nlabeled_points = rawRdd.map(lambda row: LabeledPoint(extract_label(row), extract_features(row)))\nprint labeled_points.take(5)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["This marks the completion of the feature engineering and making our feature set ready to be put into model development for classification using MLlib."],"metadata":{}},{"cell_type":"markdown","source":["# ** Part 4: Methodology **"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["Please write your answer here. Add additional IPython code/markup cells as needed. Please the grading rubric at the top of this notebook to understand expectations from this section.\n\nIn this section, explain what method you have chosen to address the chosen problem. Are you going to be using regression, classification, clustering, topic modeling, collaborative filtering, or a combination of some of these? Describe why this method is suitable for answering your problem."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["# I start my methodology with sampling out my complete dataset into 3 parts\n# randomly splitted into training data, test, data and validation data.\nweights = [.8, .1, .1]\nseed = 42\n\n# Division of complete dataset\ntrainData, valData, testData = labeled_points.randomSplit(weights, seed)\ntrainData.cache()\nvalData.cache()\ntestData.cache()\n\n# Train/test/validate dataset where I exlude reviews with UNKNOWN gender\nbinaryTrainData, binaryValData, binaryTestData = labeled_points.filter(lambda lp: lp.label != 2).randomSplit(weights, seed)\nbinaryTrainData.cache()\nbinaryValData.cache()\nbinaryTestData.cache()"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["# Function to display evaluation Metrics for our classifier\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\n\ndef evaluate_model(name, model_cls, trainData, testData, **kwargs):\n  model = model_cls.train(trainData, **kwargs)\n  predictionAndLabels = testData.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n\n  # ref: https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html\n  print(\"Summary Stats for model\")\n  labels = testData.map(lambda lp: lp.label).distinct().collect()\n  if len(labels) == 2:\n    metrics = BinaryClassificationMetrics(predictionAndLabels)\n    print(\"Area under PR = %s\" % metrics.areaUnderPR)\n    print(\"Area under ROC = %s\" % metrics.areaUnderROC)\n  else:\n    metrics = MulticlassMetrics(predictionAndLabels)\n    precision = metrics.precision()\n    recall = metrics.recall()\n    f1Score = metrics.fMeasure()\n    print(\"Precision = %s\" % precision)\n    print(\"Recall = %s\" % recall)\n    print(\"F1 Score = %s\" % f1Score)\n\n    # Statistics by class\n    for label in sorted(labels):\n      print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n      print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n      print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))\n\n    # Weighted stats\n    print(\"Weighted recall = %s\" % metrics.weightedRecall)\n    print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n    print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n    print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n    print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)\n  \n  print (\"\")\n  return model, predictionAndLabels"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["from math import log\n\n# Calculates the value of log loss for a given probabilty and label.\ndef computeLogLoss(p, y):\n    \"\"\"\n\n    Note:\n        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n        and when p is 1 we need to subtract a small value (epsilon) from it.\n\n    Args:\n        p (float): A probabilty between 0 and 1.\n        y (int): A label.  Takes on the values 0 and 1.\n\n    Returns:\n        float: The log loss value.\n    \"\"\"\n    epsilon = 10e-12\n    \n    if p == 0:\n      p = p + epsilon\n    if p == 1:\n      p = p - epsilon\n    \n    if y == 1:\n      logLoss = -log(p)\n    else:\n      logLoss = -log(1-p)\n      \n    return logLoss\n\n# Function to get raw predictions from a model. These are used to compute log loss\n# and plot ROC graphs.\ndef get_raw_predictions(model, data):\n  threshold = model.threshold\n  model.clearThreshold()\n  \n  # Force RDD to evaluate\n  rawPredictionsAndLabels = data.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n  rawPredictionsAndLabels.count()\n  rawPredictionsAndLabels.cache()\n  \n  model.setThreshold(threshold)\n  return rawPredictionsAndLabels\n\n# Function to print some basic stats about model predictions.\ndef print_stats(model, valData):\n  predictionsAndLabels = valData.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n  accuracy = predictionsAndLabels.map(lambda (pred, label): pred == label).mean()\n  \n  rawPredictionsAndLabel = get_raw_predictions(model, valData)\n  logLoss = rawPredictionsAndLabel.map(lambda (pred, label): computeLogLoss(pred, label)).mean()\n  print \"LogLoss: {} Accuracy: {}\".format(logLoss, accuracy)\n  return logLoss, accuracy"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["# Function to plot ROC curves for our selected model and on the selected dataset.\ndef plot_roc(model, valData):\n  rawPredictionsAndLabels = get_raw_predictions(model, valData)\n  labelsAndWeights = rawPredictionsAndLabels.map(lambda (pred, label): (label, pred)).collect()\n  labelsAndWeights.sort(key=lambda (k, v): v, reverse=True)\n  labelsByWeight = np.array([k for (k, v) in labelsAndWeights])\n\n  length = labelsByWeight.size\n  truePositives = labelsByWeight.cumsum()\n  numPositive = truePositives[-1]\n  falsePositives = np.arange(1.0, length + 1, 1.) - truePositives\n\n  truePositiveRate = truePositives / numPositive\n  falsePositiveRate = falsePositives / (length - numPositive)\n\n  # Generate layout and plot data\n  fig, ax = preparePlot(np.arange(0., 1.1, 0.1), np.arange(0., 1.1, 0.1))\n  ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n  ax.set_ylabel('True Positive Rate (Sensitivity)')\n  ax.set_xlabel('False Positive Rate (1 - Specificity)')\n  plt.plot(falsePositiveRate, truePositiveRate, color='#8cbfd0', linestyle='-', linewidth=3.)\n  plt.plot((0., 1.), (0., 1.), linestyle='--', color='#d6ebf2', linewidth=2.)  # Baseline model\n  return fig"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["In order to select the strongest classifier model for our resulting dataset post feature engineering, I ran a variety of different classifiers available on the scikit-learn Python library on the training dataset."],"metadata":{}},{"cell_type":"code","source":["# Logistic Regression with SGD applied on training data with only 2 classes for gender: \"FEMALE\" & \"MALE\"\nfrom pyspark.mllib.classification import LogisticRegressionWithSGD\n\nlr_model = LogisticRegressionWithSGD.train(binaryTrainData)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["# Compute the logloss and accuracy for logistic regression with SGD on Validation dataset.\nprint_stats(lr_model, binaryValData)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["# Plot ROC curve for Logistic Regression SGD\nfig = plot_roc(lr_model, binaryValData)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["# Classification model training based on SVM SGD\nfrom pyspark.mllib.classification import SVMWithSGD\n\nsvm_model = SVMWithSGD.train(binaryTrainData)\npredictionsAndLabels = binaryValData.map(lambda lp: (float(svm_model.predict(lp.features)), lp.label))\naccuracy = predictionsAndLabels.map(lambda (pred, label): pred == label).mean()\nprint \"Accuracy: {}\".format(accuracy)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["# Naive Bayes classifier model implementation on training data\nfrom pyspark.mllib.classification import NaiveBayes\n\nnb_model = NaiveBayes.train(binaryTrainData)\npredictionsAndLabels = binaryValData.map(lambda lp: (float(nb_model.predict(lp.features)), lp.label))\naccuracy = predictionsAndLabels.map(lambda (pred, label): pred == label).mean()\nprint \"Accuracy: {}\".format(accuracy)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["# Random Forest model building on training dataset to classify gender\nfrom pyspark.mllib.tree import RandomForest\n\nrf_model = RandomForest.trainClassifier(\n  binaryTrainData,\n  numClasses=2,\n  categoricalFeaturesInfo=dict((idx, 2) for idx in oheDict.itervalues()),\n  numTrees=16)\n\npredictions = rf_model.predict(binaryValData.map(lambda x: x.features))\npredictionsAndLabels = predictions.zip(binaryValData.map(lambda lp: lp.label))\naccuracy = predictionsAndLabels.map(lambda (pred, label): pred == label).mean()\nprint \"Accuracy: {}\".format(accuracy)"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["# Gradient Boosted Trees as classifier for gender classification.\nfrom pyspark.mllib.tree import GradientBoostedTrees\n\ngbt_model = GradientBoostedTrees.trainClassifier(\n  binaryTrainData, \n  categoricalFeaturesInfo=dict((idx, 2) for idx in oheDict.itervalues()),\n  numIterations=5)\n\npredictions = gbt_model.predict(binaryValData.map(lambda x: x.features))\npredictionsAndLabels = predictions.zip(binaryValData.map(lambda lp: lp.label))\naccuracy = predictionsAndLabels.map(lambda (pred, label): pred == label).mean()\nprint \"Accuracy: {}\".format(accuracy)"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["Summary of stats from different classification methods on binary dataset\n\n|Method|Log loss|Accuracy|\n|----|----|----|\n|LinearRegression|0.666|0.610|\n|SVM|na|0.610|\n|NaiveBayes|na|0.577|\n|RandomForest|na|0.610|\n|GradientBoostedTress|na|0.611|\n\nI will proceed out evaluation with LinearRegressionWithSGD since it is one of\nthe simpler model, very fast to train/test and is giving result almost as good\nas any other model."],"metadata":{}},{"cell_type":"markdown","source":["# ** Part 5: Evaluation and Results **"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["Please write your answer here. Add additional IPython code/markup cells as needed. Please the grading rubric at the top of this notebook to understand expectations from this section.\n\nIn this section, describe all experiemntal parameters such as those used for grid search on hyperparameters. Include results of the chosen methods on your task. How does the metric of interest vary with changes in important method hyperparameters such as regularization, number of iterations, etc.?"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["# Hyperparameter Tunning - Here I try to optimize LinearRegression model\n# by trying different parameters.\nimport itertools\n\nincludeIntercept = [True, False]\nregType = ['l1', 'l2']\nnumIters = [10, 100, 500]\nstepSizes = [1, 10]\nregParams = [1e-6, 1e-3]\n\ngridParams = list(itertools.product(includeIntercept, regType, numIters, stepSizes, regParams))\nprint \"Total combinations to try: \", len(gridParams)\nprint gridParams"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["# Initialize variables using values from initial model training\nbestModel = None\nbestLogLoss = 1e10\n\nfor params in gridParams:\n  includeIntercept = params[0]\n  regType = params[1]\n  numIters = params[2]\n  stepSizes = params[3]\n  regParams = params[4]\n  \n  model = LogisticRegressionWithSGD.train(\n    binaryTrainData,\n    iterations=numIters,\n    step=stepSizes,\n    regType=regType,\n    regParam=regParam,\n    intercept=includeIntercept)\n  \n  logLoss, accuracy = print_stats(model, binaryValData)\n  print ('params: {}: logloss = {:.3f}, accuracy = {:.3f}'\n    .format(params, logLoss, accuracy))\n        \n  # I choose best model by log loss\n  if (logLoss < bestLogLoss):\n    bestModel = model\n    bestLogLoss = logLoss"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["The following output is of the grid search on hyperparameters for our Best Model.\n```\nparams: (True, 'l1', 10, 1, 1e-06): logloss = 0.668, accuracy = 0.610\nparams: (True, 'l1', 10, 1, 0.001): logloss = 0.668, accuracy = 0.610\nparams: (True, 'l1', 10, 10, 1e-06): logloss = 1.163, accuracy = 0.445\nparams: (True, 'l1', 10, 10, 0.001): logloss = 1.163, accuracy = 0.445\nparams: (True, 'l1', 100, 1, 1e-06): logloss = 0.667, accuracy = 0.610\nparams: (True, 'l1', 100, 1, 0.001): logloss = 0.667, accuracy = 0.610\nparams: (True, 'l1', 100, 10, 1e-06): logloss = 1.063, accuracy = 0.610\nparams: (True, 'l1', 100, 10, 0.001): logloss = 1.063, accuracy = 0.610\nparams: (True, 'l1', 500, 1, 1e-06): logloss = 0.667, accuracy = 0.610\nparams: (True, 'l1', 500, 1, 0.001): logloss = 0.667, accuracy = 0.610\nparams: (True, 'l1', 500, 10, 1e-06): logloss = 0.666, accuracy = 0.610\nparams: (True, 'l1', 500, 10, 0.001): logloss = 0.666, accuracy = 0.610\nparams: (True, 'l2', 10, 1, 1e-06): logloss = 0.667, accuracy = 0.610\nparams: (True, 'l2', 10, 1, 0.001): logloss = 0.667, accuracy = 0.610\nparams: (True, 'l2', 10, 10, 1e-06): logloss = 1.007, accuracy = 0.457\nparams: (True, 'l2', 10, 10, 0.001): logloss = 1.007, accuracy = 0.457\nparams: (True, 'l2', 100, 1, 1e-06): logloss = 0.666, accuracy = 0.610\nparams: (True, 'l2', 100, 1, 0.001): logloss = 0.666, accuracy = 0.610\nparams: (True, 'l2', 100, 10, 1e-06): logloss = 0.826, accuracy = 0.421\nparams: (True, 'l2', 100, 10, 0.001): logloss = 0.826, accuracy = 0.421\nparams: (True, 'l2', 500, 1, 1e-06): logloss = 0.666, accuracy = 0.610\nparams: (True, 'l2', 500, 1, 0.001): logloss = 0.666, accuracy = 0.610\nparams: (True, 'l2', 500, 10, 1e-06): logloss = 0.665, accuracy = 0.609\nparams: (True, 'l2', 500, 10, 0.001): logloss = 0.665, accuracy = 0.609\nparams: (False, 'l1', 10, 1, 1e-06): logloss = 0.667, accuracy = 0.610\nparams: (False, 'l1', 10, 1, 0.001): logloss = 0.667, accuracy = 0.610\nparams: (False, 'l1', 10, 10, 1e-06): logloss = 4.440, accuracy = 0.610\nparams: (False, 'l1', 10, 10, 0.001): logloss = 4.440, accuracy = 0.610\nparams: (False, 'l1', 100, 1, 1e-06): logloss = 0.667, accuracy = 0.610\nparams: (False, 'l1', 100, 1, 0.001): logloss = 0.667, accuracy = 0.610\nparams: (False, 'l1', 100, 10, 1e-06): logloss = 0.785, accuracy = 0.443\nparams: (False, 'l1', 100, 10, 0.001): logloss = 0.785, accuracy = 0.443\nparams: (False, 'l1', 500, 1, 1e-06): logloss = 0.667, accuracy = 0.610\nparams: (False, 'l1', 500, 1, 0.001): logloss = 0.667, accuracy = 0.610\nparams: (False, 'l1', 500, 10, 1e-06): logloss = 0.666, accuracy = 0.610\nparams: (False, 'l1', 500, 10, 0.001): logloss = 0.666, accuracy = 0.610\nparams: (False, 'l2', 10, 1, 1e-06): logloss = 0.667, accuracy = 0.610\nparams: (False, 'l2', 10, 1, 0.001): logloss = 0.667, accuracy = 0.610\nparams: (False, 'l2', 10, 10, 1e-06): logloss = 4.731, accuracy = 0.610\nparams: (False, 'l2', 10, 10, 0.001): logloss = 4.731, accuracy = 0.610\nparams: (False, 'l2', 100, 1, 1e-06): logloss = 0.666, accuracy = 0.609\nparams: (False, 'l2', 100, 1, 0.001): logloss = 0.666, accuracy = 0.609\nparams: (False, 'l2', 100, 10, 1e-06): logloss = 0.924, accuracy = 0.610\nparams: (False, 'l2', 100, 10, 0.001): logloss = 0.924, accuracy = 0.610\nparams: (False, 'l2', 500, 1, 1e-06): logloss = 0.666, accuracy = 0.609\nparams: (False, 'l2', 500, 1, 0.001): logloss = 0.666, accuracy = 0.609\nparams: (False, 'l2', 500, 10, 1e-06): logloss = 0.665, accuracy = 0.609\nparams: (False, 'l2', 500, 10, 0.001): logloss = 0.665, accuracy = 0.609\n```"],"metadata":{}},{"cell_type":"code","source":["# We see that hyper parameter tuning reduced our logloss (slightly)\nfig = plot_roc(bestModel, binaryValData)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["# We known from earlier analysis that our data is highly skewed towards FEMALE. \n# Lets try assigning weights to training dataset and see if it performs better.\n# I will chose equal number of samples with male/female label and learn on that.\n\nmaleTrainData = binaryTrainData.filter(lambda lp: lp.label == 0)\nfemaleTrainData = binaryTrainData.filter(lambda lp: lp.label != 0)\nprint maleTrainData.count(), femaleTrainData.count()\n\ntrainDataWithEqualSamples = maleTrainData.union(femaleTrainData.sample(False, 1.0 * maleTrainData.count() / femaleTrainData.count()))\nprint trainDataWithEqualSamples.count()"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["# Logistic Regression with SGD classifier model on equally weighted dataset.\nmodel_with_equal_weights = LogisticRegressionWithSGD.train(\n  trainDataWithEqualSamples, \n  iterations=500, \n  step=10.0,\n  regParam=1e-03, \n  regType='l2',\n  intercept=True)\n\nprint_stats(model_with_equal_weights, binaryValData)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["# ROC plot for un-skewed data using logistic regression model.\nfig = plot_roc(model_with_equal_weights, binaryValData)\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["# Run bestModel binaryTestData and show the final result.\nprint_stats(bestModel, binaryTestData)"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["# Features with top weight in the best model. This will show which \n# features are most important to our model.\nfeatures_with_weight = list(zip(oheFeatures.collect(), bestModel.weights))\n\nfeatures_with_weight.sort(key=lambda x: -x[1], reverse=True)\ntop_features_for_male = features_with_weight[:10]\nprint \"Top feature for males:\"\nfor feature, weight in top_features_for_male:\n  print feature, weight\n\nfeatures_with_weight.sort(key=lambda x: x[1], reverse=True)\ntop_features_for_female = features_with_weight[:10]\nprint \"Top feature for females:\"\nfor feature, weight in top_features_for_female:\n  print feature, weight"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["# Analysing where our model goes wrong.\nwrong_predictions = binaryTestData.map(lambda lp: (float(bestModel.predict(lp.features)), lp)).filter(lambda (pred, lp): pred != lp.label)\nfemale_false_positives = wrong_predictions.filter(lambda (pred, lp): pred == 1.0).count()\nmale_false_positives = wrong_predictions.filter(lambda (pred, lp): pred == 0.0).count()\n\n# Very few male false positives and large number of female false positives.\nprint male_false_positives, female_false_positives"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["# Some samples of wrong predictions\nfor wrong in wrong_predictions.take(5):\n  prediction = wrong[0]\n  label = wrong[1].label\n  features = wrong[1].features\n  \n  features_with_names = list(zip(oheFeatures.collect(), features))\n  features_with_names = filter(lambda (feature, value): value == 1.0, features_with_names)\n  print \"Label: {}, Prediction: {}\".format(label, prediction)\n  for feature, values in features_with_names:\n    print feature\n  print \"\""],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":["UNSUCCESSFUL - Here is a failed attempt trying to handle skeweness of the dataset by using weights feature of the LinearRegression model in ml library. The code executed really slow and I gave up on this to not waste too much time running the model.\n\n```\nfrom pyspark.ml.linalg import Vectors, VectorUDT\nfrom pyspark.sql.types import StructType, DoubleType, StructField\n\nschema = StructType([\n    StructField(\"label\", DoubleType(), True),\n    StructField(\"features\", VectorUDT(), True)\n])\n\ndef to_ml_vector(sv):\n  return Vectors.sparse(sv.size, sv.indices, sv.values)\n\ntrain_df = spark.createDataFrame(binaryTrainData.map(lambda lp: (lp.label, to_ml_vector(lp.features))), schema)\nval_df = spark.createDataFrame(binaryValData.map(lambda lp: (lp.label, to_ml_vector(lp.features))), schema)\ntest_df = spark.createDataFrame(binaryTestData.map(lambda lp: (lp.label, to_ml_vector(lp.features))), schema)\n\n# Since we will be using TrainValidatorSplit to fit our model\n# we merge our validation and train data.\ntrain_df = train_df.unionAll(val_df)\nprint train_df.count()\n\n# We will be doing hyperparameter tuning on LogisticRegressionWithSGD since the\n# model is simple, fast to execute and has given results almost as good as any\n# other method. We use the non-deprecated LogicicRegression class.\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n\nlr = LogisticRegression()\nparamGrid = ParamGridBuilder() \\\n  .addGrid(lr.regParam, [1.0, 1e-3, 1e-6]) \\\n  .build()\n  \ntvs = TrainValidationSplit(\n  estimator=lr,\n  estimatorParamMaps=paramGrid,\n  evaluator=RegressionEvaluator(),\n  trainRatio=0.8)\n\ntuned_model = tvs.fit(train_df)\n\n# Let us check how tuning has helped\ntuned_result = tuned_model.transform(test_df)\nlog_loss = tuned_result.rdd.map(lambda row: computeLogLoss(row.probability[int(row.label)], row.label)).mean()\nprint \"log_loss: {}\".format(log_loss)\n\n# We known from earlier analysis that our data is highly skewed towards FEMALE.\n# Lets try assigning weights to training dataset and see if it performs better.\nfrom pyspark.sql import functions as F\n\ntotal_count = train_df.count()\ncount_by_gender = dict((int(k), v) for k, v in train_df.groupby(\"label\").count().collect())\nprint count_by_gender\n\nmale_ratio = 1.0 * count_by_gender[0] / total_count\nfemale_ratio = 1.0 * count_by_gender[1] / total_count\nprint male_ratio, female_ratio\n\ntrain_df_with_weights = train_df.withColumn(\"weights\", F.when(train_df.label == 0, male_ratio).otherwise(female_ratio))\nprint train_df_with_weights.head()\n\nlr_with_weights = LogisticRegression(weightCol=\"weights\")\nlr_model_with_weights = lr_with_weights.fit(train_df_with_weights)\n\n\nresult_with_weights = lr_model_with_weights.transform(test_df)\nlog_loss = result_with_weights.rdd.map(lambda row: computeLogLoss(row.probability[int(row.label)], row.label)).mean()\nprint \"log_loss: {}\".format(log_loss)\n```"],"metadata":{}},{"cell_type":"markdown","source":["# ** Part 6: Conclusions **"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["Please write your answer here. Add additional IPython code/markup cells as needed. Please the grading rubric at the top of this notebook to understand expectations from this section.\n\nThis should be a short final section stating whether the methods you explored on Yelp dataset were able to satisfactorily solve the problem you set out to solve. Discuss any business implications of the performance metrics you obtained such as accuracy, RMSE, runtime, etc. Finally, state if you think this implementation is ready for production-deployment or if there are kinks that need to be worked through before it is usable."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["## Observations\n\n* LinearRegressionWithSGD even though is a very simple model, it seems to work equally well on this problem as any other model.\n* Hyper-parameter tuning seems to help very little. As we increase the number of iterations or reduce regParam, the runtime to train the model keeps increasing.\n* Handling bias via down-sampling the majority class samples doesn't seem to help in this case.\n  * Maybe some other technique to handle skewed datasets like assigning weights to each sample, etc might work better.\n* Finally the accuracy on the test dataset is ~62.5% (slightly better than what it shows on validation dataset ~61%).\n  * Suggesting that the model is not overfitting to the train/validation dataset.\n* From the top-features it looks like businesses that are in PA, having TV (and other things listed above) seem to get lot more reviews from male while those businesses that are closed late in the night, have high rating and specify whether they take appointment or not seems to get lot more female reviews.\n* Very few male false positives and high number of female positives suggest that most of the error is because of training data skewness. If we can handle that well in our model, we can improve the accuracy of our model.\n* Another option we could have pursued is to include UNKNOWN gender data in our training dataset using one-vs-rest or other Multiclass classification techniques. \n  * For examples, to use One-vs-Rest classification technique, we could train a single classifier for each class with the samples of that class as positive (1) samples and all other samples as negatives (0). In the case of gender classification into 3 categories viz. Female, Male & Unknown, we build classifiers for each of the 3 classes and when predicting use the class with maximum score among the three classifier."],"metadata":{}},{"cell_type":"markdown","source":["In conclusion, based on the results for accuracy for our best classifier model selected, I have come up with \nMisclassification Analysis: Overall, our classifier did worse in classifying men over women. The major factors which I observed here were:\n\n* Overall, female associated business reveiws had a much higher weight than male associated business reviews. \n* Also, the population of Yelp users who actually reveiwed were highly skewed towards female gender. \n* Another reason being that apart from the two gender categories of female and male, we had significant number of business review observations which didn't have a particular gender (for those whose gender were UNKNOWN)."],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.10","nbconvert_exporter":"python","file_extension":".py"},"name":"project_yelp_dataset","notebookId":181456807238273},"nbformat":4,"nbformat_minor":0}
